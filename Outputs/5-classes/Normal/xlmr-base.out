
Number of correct predictions in the test set 4787
Val Acc1 0.7933377527345045
Number of correct predictions in the training set 4792
Epoch: 02 | Epoch Time: 1m 34s
Train Acc1 0.7941663904540934 Train Loss1 0.00010705737804528326

Number of correct predictions in the test set 4805
Val Acc1 0.7963208485250248
Number of correct predictions in the training set 4802
Epoch: 03 | Epoch Time: 1m 34s
Train Acc1 0.7958236658932714 Train Loss1 0.008325086906552315

Number of correct predictions in the test set 4856
Val Acc1 0.8047729532648326
Number of correct predictions in the training set 4854
Epoch: 04 | Epoch Time: 1m 34s
Train Acc1 0.804441498176997 Train Loss1 0.0011846309062093496

Number of correct predictions in the test set 4959
Val Acc1 0.8218428902883659
Number of correct predictions in the training set 4906
Epoch: 05 | Epoch Time: 1m 34s
Train Acc1 0.8130593304607225 Train Loss1 0.00010800666495924816

Number of correct predictions in the test set 5003
Val Acc1 0.829134902220749
-------------
End of Fold 2
                   precision    recall  f1-score   support

None-of-the-above       0.88      0.98      0.93      4787
       Homophobic       0.34      0.22      0.26       465
      Transphobic       0.00      0.00      0.00       184
      Hope-Speech       0.71      0.24      0.35       317
   Counter-speech       0.43      0.48      0.45       281

         accuracy                           0.83      6034
        macro avg       0.47      0.38      0.40      6034
     weighted avg       0.78      0.83      0.80      6034

[[4693   39    0   12   43]
 [ 256  100    0   12   97]
 [  94   64    0    2   24]
 [ 188   39    0   75   15]
 [  87   54    0    5  135]]
Fold 3
-------

Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Number of correct predictions in the training set 4763
Epoch: 01 | Epoch Time: 1m 34s
Train Acc1 0.7893602916804773 Train Loss1 0.00416842196136713

Number of correct predictions in the test set 4787
Val Acc1 0.7933377527345045
Number of correct predictions in the training set 4787
Epoch: 02 | Epoch Time: 1m 34s
Train Acc1 0.7933377527345045 Train Loss1 0.24795447289943695

Number of correct predictions in the test set 4787
Val Acc1 0.7933377527345045
Number of correct predictions in the training set 4787
Epoch: 03 | Epoch Time: 1m 34s
Train Acc1 0.7933377527345045 Train Loss1 0.16397391259670258

Number of correct predictions in the test set 4787
Val Acc1 0.7933377527345045
Number of correct predictions in the training set 4787
Epoch: 04 | Epoch Time: 1m 34s
Train Acc1 0.7933377527345045 Train Loss1 0.008812555111944675

Number of correct predictions in the test set 4787
Val Acc1 0.7933377527345045
Number of correct predictions in the training set 4787
Epoch: 05 | Epoch Time: 1m 34s
Train Acc1 0.7933377527345045 Train Loss1 0.08354753255844116

Number of correct predictions in the test set 4787
Val Acc1 0.7933377527345045
-------------
End of Fold 3
                   precision    recall  f1-score   support

None-of-the-above       0.79      1.00      0.88      4787
       Homophobic       0.00      0.00      0.00       465
      Transphobic       0.00      0.00      0.00       184
      Hope-Speech       0.00      0.00      0.00       317
   Counter-speech       0.00      0.00      0.00       281

         accuracy                           0.79      6034
        macro avg       0.16      0.20      0.18      6034
     weighted avg       0.63      0.79      0.70      6034

[[4787    0    0    0    0]
 [ 465    0    0    0    0]
 [ 184    0    0    0    0]
 [ 317    0    0    0    0]
 [ 281    0    0    0    0]]
Fold 4
-------

Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Number of correct predictions in the training set 4763
Epoch: 01 | Epoch Time: 1m 34s
Train Acc1 0.7893602916804773 Train Loss1 0.0024802752304822206

Number of correct predictions in the test set 4787
Val Acc1 0.7933377527345045
Number of correct predictions in the training set 4788
Epoch: 02 | Epoch Time: 1m 34s
Train Acc1 0.7935034802784222 Train Loss1 4.426129817147739e-05

Number of correct predictions in the test set 4823
Val Acc1 0.7993039443155452
Number of correct predictions in the training set 4807
Epoch: 03 | Epoch Time: 1m 34s
Train Acc1 0.7966523036128604 Train Loss1 0.004121529404073954

Number of correct predictions in the test set 4867
Val Acc1 0.8065959562479283
Number of correct predictions in the training set 4846
Epoch: 04 | Epoch Time: 1m 34s
Train Acc1 0.8031156778256545 Train Loss1 4.806942342838738e-06

Number of correct predictions in the test set 4898
Val Acc1 0.8117335101093801
Number of correct predictions in the training set 4888
Epoch: 05 | Epoch Time: 1m 34s
Train Acc1 0.8100762346702022 Train Loss1 3.292375185992569e-05

Number of correct predictions in the test set 4958
Val Acc1 0.821677162744448
-------------
End of Fold 4
                   precision    recall  f1-score   support

None-of-the-above       0.88      0.98      0.93      4787
       Homophobic       0.33      0.25      0.29       465
      Transphobic       0.42      0.12      0.19       184
      Hope-Speech       0.72      0.07      0.12       317
   Counter-speech       0.43      0.43      0.43       281

         accuracy                           0.82      6034
        macro avg       0.56      0.37      0.39      6034
     weighted avg       0.79      0.82      0.79      6034

[[4677   66    1    3   40]
 [ 232  118   19    4   92]
 [  77   73   22    0   12]
 [ 232   43    7   21   14]
 [ 100   57    3    1  120]]
Fold 5
-------

Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Number of correct predictions in the training set 4787
Epoch: 01 | Epoch Time: 1m 34s
Train Acc1 0.7933377527345045 Train Loss1 0.0003924289776477963

Number of correct predictions in the test set 4787
Val Acc1 0.7933377527345045
Number of correct predictions in the training set 4795
Epoch: 02 | Epoch Time: 1m 34s
Train Acc1 0.7946635730858468 Train Loss1 0.18580392003059387

Number of correct predictions in the test set 4808
Val Acc1 0.7968180311567782
Number of correct predictions in the training set 4801
Epoch: 03 | Epoch Time: 1m 34s
Train Acc1 0.7956579383493536 Train Loss1 0.06691926717758179

Number of correct predictions in the test set 4864
Val Acc1 0.806098773616175
Number of correct predictions in the training set 4851
Epoch: 04 | Epoch Time: 1m 34s
Train Acc1 0.8039443155452436 Train Loss1 0.09831703454256058

Number of correct predictions in the test set 4884
Val Acc1 0.809413324494531
Number of correct predictions in the training set 4880
Epoch: 05 | Epoch Time: 1m 34s
Train Acc1 0.8087504143188597 Train Loss1 2.024483819695888e-06

Number of correct predictions in the test set 4930
Val Acc1 0.8170367915147497
-------------
End of Fold 5
                   precision    recall  f1-score   support

None-of-the-above       0.87      0.98      0.92      4787
       Homophobic       0.39      0.22      0.28       465
      Transphobic       0.00      0.00      0.00       184
      Hope-Speech       0.00      0.00      0.00       317
   Counter-speech       0.41      0.54      0.47       281

         accuracy                           0.82      6034
        macro avg       0.33      0.35      0.33      6034
     weighted avg       0.74      0.82      0.77      6034

[[4677   58    0    0   52]
 [ 250  102    0    0  113]
 [ 105   43    0    0   36]
 [ 278   23    0    0   16]
 [  93   37    0    0  151]]
